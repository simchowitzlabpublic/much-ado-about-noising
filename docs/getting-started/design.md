# Architecture & Design

This page provides an in-depth look at this repository's architecture and design principles.

## Overview

This repository is built on the [stochastic interpolant framework](https://arxiv.org/abs/2303.08797), a unified mathematical formulation that encompasses flow matching, consistency models, and various distillation methods.

## Stochastic Interpolant

The stochastic interpolant framework provides a unified view of generative modeling:

```
a_t = Î±(t) * a_0 + Î²(t) * a_1
```

Where:
- `a_t` interpolates between action at time `t=0` and action at time `t=1`
- `Î±(t)`, `Î²(t)` are interpolation schedules (linear or trigonometric)
- The goal is to learn a velocity field `v(a, t)` that transforms action at time `t=0` to action at time `t=1`
  - `v(a, t) = d/dt a(t)`

## Algorithms Overview

### **Flow Matching** (`flow`)

https://arxiv.org/abs/2210.02747

Standard flow matching loss.

**Train:**

```
b_Î¸ â‰ˆ argmin_Î¸ ğ”¼[â€–b_t(I_t | o) - Ä°_tâ€–Â²],  where t ~ Unif([0,1]), z ~ N(0, I)
```

**Inference:**

```
d/dt a_t = b_t(a_t | o)  with initial condition  a_0 = z
```

### **Regression** (`regression`)

Direct regression loss, learns conditional action mean.

**Train:**

```
Ï€_Î¸ â‰ˆ argmin_Î¸ ğ”¼[â€–Ï€_Î¸(o, I_0, t=0) - aâ€–Â²]
```

**Inference:**

```
Ã¢ â† Ï€_Î¸(o, z, t=0)
```

### **Two Step Denoising** (`tsd`)

Two step denoising, a simplified version of flow matching.

**Train:**

```
Ï€_Î¸ â‰ˆ argmin_Î¸ ğ”¼[â€–Ï€_Î¸(o, I_0, t=0) - I_fixâ€–Â² + â€–Ï€_Î¸(o, I_fix, t_fix) - aâ€–Â²]
```

**Inference:**

```
Ã¢_0 â† Ï€_Î¸(o, z, 0)
Ã¢ â† Ï€_Î¸(o, t_fix * Ã¢_0 + (1 - t_fix) * z, t_fix)
```

### **Minimum Iterative Policy** (`mip`)

Minimum Iterative Policy, optimized for two-step sampling by removing redundant stochasticity in input.

**Train:**

```
Ï€_mip^Î¸ â‰ˆ argmin_Î¸ ğ”¼[â€–Ï€_Î¸(o, I_0 = 0, t=0) - aâ€–Â² + â€–Ï€_Î¸(o, I_t_fix, t_fix) - aâ€–Â²]
```

**Inference:**

```
Ã¢_mip^0 â† Ï€_mip^Î¸(o, 0, t=0)
Ã¢_mip â† Ï€_mip^Î¸(o, t_fix * Ã¢_mip^0, t_fix)
```

### **Consistency Trajectory Model** (`ctm`)

https://arxiv.org/abs/2310.02279

Consistency Trajectory Model, progressively distills multi-step flow into flow map/shortcut models.

**Train:**

```
Î¦_{s,t}(I_s) â‰ˆ Î¦_{s+dt, t}(stopgrad(Î¦_{s, s+dt}(I_s)))
```

**Inference:**

```
a = Î¦_{0,1}(z),  where z ~ N(0, I)
```

### **Progressive Self-Distillation** (`psd`)

https://arxiv.org/pdf/2505.18825

Progressive Self-Distillation, a self-distillation framework which trains a flow map.

**Train:**

```
L_SD = L_b + L_D
L_b = â€–b_t(I_t) - Ä°_tâ€–Â²
L_D = â€–Î¦_{s,t}(I_s) - Î¦_{u,t}(Î¦_{s,u}(I_s))â€–Â²
```

**Inference:**

```
Ã¢ = Î¦_{0,1}(z),  where z ~ N(0, I)
```

### **Lagrangian Self-Distillation** (`lsd`)

https://arxiv.org/abs/2505.18825

Lagrangian Self-Distillation, a self-distillation framework which trains a flow map.

**Train:**

```
L_SD = L_b + L_D
L_b = â€–b_t(I_t) - Ä°_tâ€–Â²
L_D = â€–âˆ‚_t Î¦_{s,t}(I_s) - b_t(Î¦_{s,t}(I_s))â€–Â²
```

**Inference:**

```
Ã¢ = Î¦_{0,1}(z),  where z ~ N(0, I)
```

### **Euler Self-Distillation** (`esd`)

https://arxiv.org/abs/2505.18825

Euler Self-Distillation, a self-distillation framework which trains a flow map.

**Train:**

```
L_SD = L_b + L_D
L_b = â€–b_t(I_t) - Ä°_tâ€–Â²
L_D = â€–âˆ‚_s Î¦_{s,t}(I_s) + âˆ‡ Î¦_{s,t}(I_s) Â· b_s(I_s)â€–Â²
```

**Inference:**

```
Ã¢ = Î¦_{0,1}(z),  where z ~ N(0, I)
```

### **Mean Flow** (`mf`)

https://arxiv.org/abs/2505.13447

Mean Flow, a self-distillation framework which trains a flow map.

**Define:**

```
Î¦_{s,t}(I_s) = I_s + (t - s) * vÌ„_{s,t}(I_s)
```

**Train:**

```
L_SD = L_b + L_D
L_b = â€–b_t(I_t) - Ä°_tâ€–Â²
L_D = â€–âˆ‚_s Î¦_{s,t}(I_s) + stopgrad(âˆ‡ Î¦_{s,t}(I_s) Â· Ä°_s)â€–Â²
```

**Inference:**

```
Ã¢ = Î¦_{0,1}(z),  where z ~ N(0, I)
```
